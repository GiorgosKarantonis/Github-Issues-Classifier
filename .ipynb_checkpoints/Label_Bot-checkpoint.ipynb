{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "colab_type": "code",
    "id": "pT91mvT8Xcei",
    "outputId": "fcbf2b19-204d-4543-d740-4b4596ae23f0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import preprocessing as pp\n",
    "\n",
    "from transformers import DistilBertConfig\n",
    "from transformers import DistilBertTokenizer, DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertModel, TFDistilBertForSequenceClassification\n",
    "\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_CPf63QXKTq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bug', 'enhancement', 'question']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MEMORY_LIMIT = 1000\t# set to None if you don't want to apply any limit\n",
    "\n",
    "LABELS = [\n",
    "\t'bug', \n",
    "\t'enhancement', \n",
    "\t'question'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "colab_type": "code",
    "id": "pR-IXJDyXYT-",
    "outputId": "dbe4f689-3e05-44b0-c757-5901a072b0fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bug            406\n",
      "enhancement    306\n",
      "question        96\n",
      "help wanted     30\n",
      "feature         26\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pp.load_data(memory_limit=MEMORY_LIMIT)\n",
    "# labels = [df[c] for c in df if c.startswith('label_')]\n",
    "\n",
    "pp.get_unique_values(df, 'labels').head()\n",
    "\n",
    "for c in df.columns:\n",
    "\tif c.startswith('label_') and c.strip('label_') not in LABELS:\n",
    "\t\tdf = df.drop(c, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0kHpzwYaQ0a3"
   },
   "outputs": [],
   "source": [
    "def get_paraphrased_values(df, feature='labels'):\n",
    "\tunique_labels = pp.get_unique_values(df, feature).keys().values\n",
    "\tcombinations = np.array(list(itertools.combinations(unique_labels, 2))).tolist()\n",
    "\n",
    "\ttokenizer = BertTokenizer.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "\tmodel = TFBertForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
    "\n",
    "\tinputs = tokenizer(\tcombinations, \n",
    "                      padding=True, \n",
    "                      truncation=True, \n",
    "                      return_tensors='tf')\n",
    "\n",
    "\tlogits = model(inputs)[0]\n",
    "\toutputs = tf.nn.softmax(logits, axis=1)\n",
    "\n",
    "\treturn outputs.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stHHPo7uXmEC"
   },
   "outputs": [],
   "source": [
    "test = get_paraphrased_values(df)\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQtYzY_rQ6Z5"
   },
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# # Check here for multi-label classification\n",
    "# # https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
    "# # https://github.com/huggingface/transformers/issues/1465\n",
    "# # https://towardsdatascience.com/multi-label-classification-using-bert-roberta-xlnet-xlm-and-distilbert-with-simple-transformers-b3e0cda12ce5\n",
    "\n",
    "\n",
    "# configuration = DistilBertConfig(num_labels=len(labels))\n",
    "\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "# model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=configuration)\n",
    "\n",
    "\n",
    "# # inputs_title = tokenizer(\tdf['title'].values.tolist(), \n",
    "# # \t\t\t\t\t\t\tpadding=True, \n",
    "# # \t\t\t\t\t\t\ttruncation=True, \n",
    "# # \t\t\t\t\t\t\treturn_tensors='tf')\n",
    "\n",
    "# inputs_body = tokenizer(\tdf['body'].values.tolist(), \n",
    "# \t\t\t\t\t\t\tpadding=True, \n",
    "# \t\t\t\t\t\t\ttruncation=True, \n",
    "# \t\t\t\t\t\t\treturn_tensors='tf')\n",
    "\n",
    "# # inputs_body['labels'] = tf.transpose(tf.convert_to_tensor(labels))\n",
    "# inputs_body['labels'] = tf.convert_to_tensor(np.array(labels).reshape(-1, 1))\n",
    "\n",
    "\n",
    "# outputs = model(inputs_body)\n",
    "\n",
    "# print(outputs)\n",
    "\n",
    "# loss, logits = outputs[:2]\n",
    "\n",
    "# print(logits.shape)\n",
    "# print(loss)\n",
    "\n",
    "# print(f'Total time elapsed: {time.time() - start}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Label Bot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
