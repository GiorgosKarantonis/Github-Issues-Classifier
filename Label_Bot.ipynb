{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Label Bot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0oWJJZARFP8",
        "colab_type": "text"
      },
      "source": [
        "# **1. Prepare the environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8rrR4mFRWWX",
        "colab_type": "text"
      },
      "source": [
        "## **1.1 Fetch the \"Label-Bot\" repo and modify it so that it can be used in colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FufcnMRQ75Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/GiorgosKarantonis/Label-Bot\n",
        "\n",
        "!mv Label-Bot Label_Bot\n",
        "!touch Label_Bot/__init__.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBsroC2GRnLy",
        "colab_type": "text"
      },
      "source": [
        "## **1.2 Mount google drive in order to be able to access the preprocessed dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zFENsN0ROgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RppdICi9R4Kx",
        "colab_type": "text"
      },
      "source": [
        "# **2. Prepare the Label Bot**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCai1FSsSFa6",
        "colab_type": "text"
      },
      "source": [
        "## **2.1 Import all the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT91mvT8Xcei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import Label_Bot.preprocessing as pp\n",
        "\n",
        "try:\n",
        "    from transformers import BertTokenizer, DistilBertTokenizer, DistilBertTokenizerFast\n",
        "    from transformers import TFBertModel, TFDistilBertModel\n",
        "    from transformers import TFBertForSequenceClassification, TFDistilBertForSequenceClassification\n",
        "except:\n",
        "    !pip install transformers==3.0.0\n",
        "\n",
        "    from transformers import BertTokenizer, DistilBertTokenizer, DistilBertTokenizerFast\n",
        "    from transformers import TFBertModel, TFDistilBertModel\n",
        "    from transformers import TFBertForSequenceClassification, TFDistilBertForSequenceClassification"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iWmKUq9SKN8",
        "colab_type": "text"
      },
      "source": [
        "## **2.2 Define the hyperparameters and the global variables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_CPf63QXKTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEMORY_LIMIT = 100\t# set to None if you don't want to apply any limit\n",
        "\n",
        "# a dummy set of labels used for the proof of concept\n",
        "LABELS = [\n",
        "\t'bug', \n",
        "\t'enhancement', \n",
        "\t'question'\n",
        "]\n",
        "\n",
        "# an actual set of labels that the bot should be able to predict\n",
        "EXAMPLE_LABELS = [\n",
        "\t'bug', \n",
        "\t'enhancement', \n",
        "\t'documentation', \n",
        "\t'duplicate', \n",
        "\t'maintenance', \n",
        "\t'good first issue', \n",
        "\t'help wanted', \n",
        "\t'invalid', \n",
        "\t'question', \n",
        "\t\"won't fix\", \n",
        "\t'status: proposal', \n",
        "\t'status: available', \n",
        "\t'status: in progress', \n",
        "\t'status: on hold', \n",
        "\t'status: blocked', \n",
        "\t'status: abandoned', \n",
        "\t'status: review needed', \n",
        "\t'priority: low', \n",
        "\t'priority: medium', \n",
        "\t'priority: high', \n",
        "\t'priority: critical'\n",
        "]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQJNjETLSisM",
        "colab_type": "text"
      },
      "source": [
        "## **2.3 Load the preprocessed dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR-IXJDyXYT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b5b79d39-7859-4d31-d991-ddd64f87e9e5"
      },
      "source": [
        "df = pp.load_data(memory_limit=MEMORY_LIMIT, file='./drive/My Drive/Label Bot/data/github.pkl')\n",
        "labels = np.transpose([df[c] for c in df if c.startswith('label_')])\n",
        "\n",
        "for c in df.columns:\n",
        "    if c.startswith('label_') and c.replace('label_', '') not in LABELS:\n",
        "        df = df.drop(c, axis=1)\n",
        "\n",
        "print(pp.get_unique_values(df, 'labels').head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bug            56\n",
            "enhancement    18\n",
            "question        9\n",
            "fixed           3\n",
            "kind/bug        3\n",
            "Name: labels, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_SG9iriSxfK",
        "colab_type": "text"
      },
      "source": [
        "# **3 Use the Pre-Trained Models** **bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk_eSJnUKYyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "tokenizer_dstl = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer_dstl_fast = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "model = TFBertModel.from_pretrained('bert-base-cased')\n",
        "model_dstl = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "model_cls = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
        "model_cls_dstl = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h35pU9YvAzX4",
        "colab_type": "text"
      },
      "source": [
        "## **3.1 Disambiguate the Labels**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kHpzwYaQ0a3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_paraphrase(inputs):\n",
        "\ttokenizer = BertTokenizer.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
        "\tmodel = TFBertForSequenceClassification.from_pretrained('bert-base-cased-finetuned-mrpc')\n",
        "\n",
        "\tinputs = tokenizer(\tinputs, \n",
        "\t\t\t\t\t\tpadding=True, \n",
        "\t\t\t\t\t\ttruncation=True, \n",
        "\t\t\t\t\t\treturn_tensors='tf')\n",
        "\n",
        "\tlogits = model(inputs)[0]\n",
        "\toutputs = tf.nn.softmax(logits, axis=1).numpy()\n",
        "\n",
        "\tnot_paraphrase_likelihood = outputs[:, 0]\n",
        "\tparaphrase_likelihood = outputs[:, 1]\n",
        "\n",
        "\treturn not_paraphrase_likelihood, paraphrase_likelihood"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNYaKTCRCjL9",
        "colab_type": "text"
      },
      "source": [
        "### **3.1.1 Test the similarity between all the labels that are present in the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stHHPo7uXmEC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ec14b65-b447-431f-8e06-f65a5aa76fb8"
      },
      "source": [
        "unique_labels = pp.get_unique_values(df, 'labels').keys().values\n",
        "combinations = np.array(list(itertools.combinations(unique_labels, 2))).tolist()\n",
        "\n",
        "_, paraphrase_likelihood = check_paraphrase(combinations)\n",
        "\n",
        "for i, pair in enumerate(combinations):\n",
        "\tif paraphrase_likelihood[i] > .8:\n",
        "\t\tprint(f'{pair[0]}\\t{pair[1]}\\t\\t{paraphrase_likelihood[i]}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased-finetuned-mrpc and are newly initialized: ['dropout_37']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "bug\tkind/bug\t\t0.848659336566925\n",
            "bug\ttype: bug\t\t0.8241774439811707\n",
            "enhancement\tdoc-enhancement\t\t0.8313372731208801\n",
            "enhancement\tkind/enhancement\t\t0.9053848385810852\n",
            "kind/bug\ttype: bug\t\t0.9100732803344727\n",
            "kind/bug\tbug report\t\t0.8221029043197632\n",
            "kind/bug\ttype:visual bug\t\t0.8794167041778564\n",
            "kind/bug\tkind/enhancement\t\t0.8732624650001526\n",
            "kind/bug\ttype-bug\t\t0.9201246500015259\n",
            "kind/bug\textension-bug\t\t0.8677968978881836\n",
            "kind/bug\tkind/bug/p0\t\t0.9146767854690552\n",
            "in progress\tin-progress\t\t0.9327884912490845\n",
            "testing\ttests\t\t0.8916923403739929\n",
            "type: bug\ttype:feature\t\t0.9392443299293518\n",
            "type: bug\tbug report\t\t0.8869650959968567\n",
            "type: bug\tminor bug\t\t0.8080185651779175\n",
            "type: bug\ttype:visual bug\t\t0.9418724775314331\n",
            "type: bug\tpossible-bug\t\t0.8296520113945007\n",
            "type: bug\ttype-bug\t\t0.9411540627479553\n",
            "type: bug\tbrowser: chrome\t\t0.9087804555892944\n",
            "type: bug\ttest-bug\t\t0.8598078489303589\n",
            "type: bug\textension-bug\t\t0.9007737040519714\n",
            "hard problem\teasy\t\t0.8164892196655273\n",
            "type:feature\tfeature\t\t0.8799180388450623\n",
            "type:feature\tpriority: medium\t\t0.8643220663070679\n",
            "type:feature\ttype:visual bug\t\t0.9231083989143372\n",
            "type:feature\ttype-bug\t\t0.923528790473938\n",
            "type:feature\tbrowser: chrome\t\t0.9184271693229675\n",
            "bug report\ttype-bug\t\t0.8302042484283447\n",
            "area-specification\tarea: power management\t\t0.872900128364563\n",
            "area-specification\tarea/documentation\t\t0.8994398713111877\n",
            "area-specification\tarea: arc\t\t0.8798635005950928\n",
            "minor bug\ttype:visual bug\t\t0.8033540844917297\n",
            "minor bug\ttype-bug\t\t0.8485909700393677\n",
            "graph\teasy\t\t0.813260018825531\n",
            "gtk3\ts3\t\t0.8482204079627991\n",
            "needs review\tneeds update\t\t0.8675494194030762\n",
            "v4\ts3\t\t0.8752257227897644\n",
            "priority: medium\ttype:visual bug\t\t0.869158923625946\n",
            "priority: medium\tpriority: low\t\t0.9231568574905396\n",
            "priority: medium\tbrowser: chrome\t\t0.8143224120140076\n",
            "code example\tcode\t\t0.913070023059845\n",
            "area: power management\tarea: arc\t\t0.9133169651031494\n",
            "type:visual bug\tpossible-bug\t\t0.834082841873169\n",
            "type:visual bug\ttype-bug\t\t0.9388346672058105\n",
            "type:visual bug\tbrowser: chrome\t\t0.9063917994499207\n",
            "type:visual bug\ttest-bug\t\t0.8647027611732483\n",
            "type:visual bug\textension-bug\t\t0.891149640083313\n",
            "tests\ttest-bug\t\t0.8325897455215454\n",
            "priority: low\thigh priority\t\t0.9182889461517334\n",
            "doc-enhancement\tkind/enhancement\t\t0.8081086277961731\n",
            "possible-bug\ttype-bug\t\t0.9227825999259949\n",
            "possible-bug\ttest-bug\t\t0.920874834060669\n",
            "possible-bug\textension-bug\t\t0.8653614521026611\n",
            "p3\ts3\t\t0.9208282232284546\n",
            "type-bug\ttest-bug\t\t0.9233443140983582\n",
            "type-bug\textension-bug\t\t0.9337631464004517\n",
            "test-bug\textension-bug\t\t0.8810417056083679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU80dNvvCt3R",
        "colab_type": "text"
      },
      "source": [
        "### **3.1.2 Test the similarity between the predefined labels and the labels that are present in the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKypfgDI5t8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "5f51b450-211a-4d26-819f-9fe1644407fd"
      },
      "source": [
        "test_list = []\n",
        "\n",
        "for target_l in LABELS:\n",
        "    for real_l in unique_labels:\n",
        "        test_list.append([target_l, real_l])\n",
        "\n",
        "_, paraphrase_likelihood = check_paraphrase(test_list)\n",
        "\n",
        "for i, pair in enumerate(test_list):\n",
        "\tif paraphrase_likelihood[i] > .5:\n",
        "\t\tprint(f'{pair[0]}\\t{pair[1]}\\t\\t{paraphrase_likelihood[i]}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased-finetuned-mrpc and are newly initialized: ['dropout_341']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "bug\tbug\t\t0.9162982702255249\n",
            "bug\tkind/bug\t\t0.848659336566925\n",
            "bug\ttype: bug\t\t0.8241774439811707\n",
            "bug\tbug report\t\t0.6632633209228516\n",
            "bug\tminor bug\t\t0.5306739211082458\n",
            "bug\ttype:visual bug\t\t0.7715802192687988\n",
            "bug\ttype-bug\t\t0.7991790175437927\n",
            "bug\textension-bug\t\t0.6228403449058533\n",
            "enhancement\tenhancement\t\t0.9370464086532593\n",
            "enhancement\tdoc-enhancement\t\t0.8313372731208801\n",
            "enhancement\tkind/enhancement\t\t0.9053848385810852\n",
            "question\tquestion\t\t0.9274783134460449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AEfYPmE__VU",
        "colab_type": "text"
      },
      "source": [
        "## **3.2 Get the Contextual Word Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Z5uX5Z1Ynu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embeddings(data, tokenizer=tokenizer_dstl, model=model_dstl):\n",
        "    inputs = tokenizer(\tdata, \n",
        "                        padding=True, \n",
        "                        truncation=True, \n",
        "                        return_tensors='tf')\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    logits = outputs[0]\n",
        "\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwObheGj2Ch7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_title = get_embeddings(df['title'].values.tolist())  # n_examples x 768\n",
        "embedding_body = get_embeddings(df['body'].values.tolist())  # n_examples x 768"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJrIMfVA_2I",
        "colab_type": "text"
      },
      "source": [
        "## **3.3 Run the Pre-Trained Sequence Classification Model**\n",
        "\n",
        "Actually don't run it, it will fail. \n",
        "\n",
        "The pretrained model cannot handle Multilabel Classification tasks. It can be used in another way though... Since the architecture won't rely solely on this model, it can be used to boost the training by: \n",
        "\n",
        "- Select randomly one target class when two or more are present. \n",
        "- Use all target classes when 2 or more are present but split them into different examples and add random noise. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgVUiD5MLG4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seq_classification(data, labels, tokenizer=tokenizer_dstl, model=model_cls_dstl):\n",
        "    inputs = tokenizer(\tdata, \n",
        "                        padding=True, \n",
        "                        truncation=True, \n",
        "                        return_tensors='tf')\n",
        "\n",
        "    inputs['labels'] = tf.convert_to_tensor(np.array(labels).reshape(-1, 1))\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    return loss, logits"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVgPnu2xFHTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_num = []  # 0: no label , i+1: for any other defined in LABELS\n",
        "for row in labels:\n",
        "    vector_to_num = 0\n",
        "    for j, column_value in enumerate(row):\n",
        "        vector_to_num += (j + 1) * column_value\n",
        "    \n",
        "    labels_num.append(vector_to_num)\n",
        "\n",
        "loss_cls_title, logits_cls_title = seq_classification(df['title'].values.tolist(), labels_num)  # loss: n_examples x 1 , logits: n_examples x n_labels\n",
        "loss_cls_body, logits_cls_body = seq_classification(df['body'].values.tolist(), labels_num)  # loss: n_examples x 1 , logits: n_examples x n_labels"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}